{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APSIM Database Files Conversion\n",
    "\n",
    "This notebook provides tools and workflows for converting and processing APSIM database files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports successful\n",
      "Note: pandas is required for CSV conversion\n",
      "Install with: pip install pandas\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import pandas as pd\n",
    "\n",
    "print(\"‚úì Imports successful\")\n",
    "print(\"Note: pandas is required for CSV conversion\")\n",
    "print(\"Install with: pip install pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Schema\n",
    "\n",
    "Each APSIM database file contains two main tables with simulation outputs:\n",
    "\n",
    "### Report Table (End-of-Season Information)\n",
    "\n",
    "The Report table contains annual/seasonal summary data with the following fields:\n",
    "\n",
    "| Field | Description |\n",
    "|-------|-------------|\n",
    "| **Year** | Simulation year |\n",
    "| **Latitude** | Geographic latitude |\n",
    "| **Longitude** | Geographic longitude |\n",
    "| **Cultivar** | Crop cultivar/variety identifier |\n",
    "| **CropSurvived** | Logical output indicating if the crop survived the season. If `False` (or 0), the model 'killed' the crop. Note: zero yield could occur for other reasons than a dead crop. |\n",
    "| **SowingDate** | Date when crop was sown |\n",
    "| **CurrentPhenologyStage** | Final phenology stage reached |\n",
    "| **FloweringDaysAfterSowing** | Number of days from sowing to flowering |\n",
    "| **MaturityDaysAfterSowing** | Number of days from sowing to maturity |\n",
    "| **HarvestDate** | Date when crop was harvested |\n",
    "| **Yield** | Dry weight yield in kg/ha (Note: column name is \"Yield\", not \"DryYield\") |\n",
    "| **WetYield** | Wet weight yield in kg/ha (grain moisture of 12%) |\n",
    "| **FrostHeatYield** | Yield in kg/ha affected by frost and heat extreme events. If this equals Yield, then no frost or heat effect occurred. |\n",
    "| **GrainProtein** | Grain protein percentage (will always be high with UNLIMITED N in these simulations) |\n",
    "| **InCropRain** | Rainfall (mm) between sowing and harvest day |\n",
    "| **FallowRain** | Rainfall (mm) before sowing |\n",
    "| **SowingPAW** | Plant Available Water (mm) at sowing time |\n",
    "| **HarvestPAW** | Plant Available Water (mm) at harvest time |\n",
    "| **PAWC** | Soil Plant Available Water Capacity (mm) |\n",
    "| **CumulativeFrost** | Percentage frost impact on yield |\n",
    "| **CumulativeHeat** | Percentage heat impact on yield |\n",
    "| **NumFrostEvents** | Number of frost events during the season |\n",
    "| **NumHeatEvents** | Number of heat events during the season |\n",
    "\n",
    "### Daily Table (Daily Time Series)\n",
    "\n",
    "The Daily table contains daily outputs from the simulations with the following fields:\n",
    "\n",
    "| Field | Description |\n",
    "|-------|-------------|\n",
    "| **Date** | Date of the simulation day |\n",
    "| **Year** | Simulation year |\n",
    "| **Latitude** | Geographic latitude |\n",
    "| **Longitude** | Geographic longitude |\n",
    "| **Cultivar** | Crop cultivar/variety identifier |\n",
    "| **CropSurvived** | Logical output indicating if the crop survived. If `False` (or 0), the model 'killed' the crop. Note: zero yield could occur for other reasons than a dead crop. |\n",
    "| **SowingDate** | Date when crop was sown |\n",
    "| **CurrentPhenologyStage** | Current phenology stage on this day |\n",
    "| **DryYield** | Dry weight yield in kg/ha (cumulative) |\n",
    "| **WetYield** | Wet weight yield in kg/ha, considering grain moisture of 12% (cumulative) |\n",
    "| **FrostHeatYield** | Yield in kg/ha affected by frost and heat extreme events. If this equals DryYield, then no frost or heat effect occurred. |\n",
    "| **GrainProtein** | Grain protein percentage (will always be high with UNLIMITED N in these simulations) |\n",
    "| **CumulativeFrost** | Percentage frost impact on yield (cumulative) |\n",
    "| **CumulativeHeat** | Percentage heat impact on yield (cumulative) |\n",
    "| **RadiationsFactor** | Radiation factor for the crop (0-1), where 1 = no stress (Note: column name is \"RadiationsFactor\" with an 's', not \"RadiationFactor\") |\n",
    "| **WaterFactor** | Water factor (0-1), where 1 = no water stress |\n",
    "| **NitrogenFactor** | Nitrogen factor (0-1), where 1 = no nitrogen stress |\n",
    "| **VapourDeficitFactor** | Vapour deficit factor (0-1), where 1 = no vapour deficit stress |\n",
    "\n",
    "### Additional Tables\n",
    "\n",
    "The database may also contain metadata tables:\n",
    "- **\\_Simulations**: Simulation metadata\n",
    "- **\\_Checkpoints**: Checkpoint information\n",
    "- **\\_Messages**: Warnings, errors, and informational messages\n",
    "- **\\_Units**: Unit definitions\n",
    "- **\\_InitialConditions**: Initial soil conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Configuration\n",
      "================================================================================\n",
      "Base directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\n",
      "Directory exists: True\n",
      "\n",
      "Mode: SPECIFIC FILE\n",
      "Farm name: Anameka\n",
      "Coordinate: -31.45_117.55\n",
      "Database file: ClimAdapt_Wheat_neg31.45_117.55_585_calibrated\n",
      "\n",
      "Full path: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\-31.45_117.55_APSIM\\ClimAdapt_Wheat_neg31.45_117.55_585_calibrated\n",
      "File exists: False\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BASE_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\"\n",
    "\n",
    "# Option 1: Specify a specific database file (set these values)\n",
    "FARM_NAME = \"Anameka\"  # e.g., \"Anameka\"\n",
    "COORDINATE = \"-31.45_117.55\"  # e.g., \"-31.45_117.55\"\n",
    "DB_FILE_NAME = \"ClimAdapt_Wheat_neg31.45_117.55_585_calibrated\"  # include the\".db\"\n",
    "\n",
    "# Option 2: Leave DB_FILE_NAME = None to discover all database files\n",
    "\n",
    "# Set your base directory here (should contain farm name folders)\n",
    "base_directory = Path(BASE_DIR)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Configuration\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Base directory: {base_directory}\")\n",
    "print(f\"Directory exists: {base_directory.exists()}\")\n",
    "\n",
    "if DB_FILE_NAME:\n",
    "    # Specific file mode\n",
    "    print(f\"\\nMode: SPECIFIC FILE\")\n",
    "    print(f\"Farm name: {FARM_NAME}\")\n",
    "    print(f\"Coordinate: {COORDINATE}\")\n",
    "    print(f\"Database file: {DB_FILE_NAME}\")\n",
    "    \n",
    "    # Construct the full path: base_directory / FARM_NAME / COORDINATE_APSIM / DB_FILE_NAME\n",
    "    COORDINATE_APSIM = f\"{COORDINATE}_APSIM\"  # e.g., \"-31.45_117.55_APSIM\"\n",
    "    specific_db_path = base_directory / FARM_NAME / COORDINATE_APSIM / DB_FILE_NAME\n",
    "    print(f\"\\nFull path: {specific_db_path}\")\n",
    "    print(f\"File exists: {specific_db_path.exists()}\")\n",
    "    \n",
    "    # Set mode flag\n",
    "    USE_SPECIFIC_FILE = True\n",
    "    target_db_file = specific_db_path if specific_db_path.exists() else None\n",
    "else:\n",
    "    # Discovery mode\n",
    "    print(f\"\\nMode: DISCOVER ALL FILES\")\n",
    "    print(\"Will search for all .db files in the nested structure\")\n",
    "    USE_SPECIFIC_FILE = False\n",
    "    target_db_file = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database File Discovery\n",
    "\n",
    "Find database file(s) based on configuration:\n",
    "- **Specific file mode**: Uses the configured `FARM_NAME`, `COORDINATE`, and `DB_FILE_NAME`\n",
    "- **Discovery mode**: Finds all `.db` files in the nested structure: `{base_path}/{farm_name}/{coordinate}_APSIM/*.db` (e.g., `{base_path}/{farm_name}/-31.45_117.55_APSIM/*.db`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Error: Specific File Mode\n",
      "================================================================================\n",
      "\n",
      "‚úó Invalid path configuration\n",
      "\n",
      "Please check:\n",
      "  - Base directory: C:\\Users\\ibian\\Desktop\\ClimAdapt (exists: True)\n",
      "  - Farm name: Anameka\n",
      "  - Coordinate: -31.45_117.55\n",
      "  - Database file name: ClimAdapt_Wheat_neg31.45_117.55_585_calibrated\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Find database file(s) based on configuration\n",
    "db_files_by_location = {}\n",
    "total_files = 0\n",
    "all_db_files = []\n",
    "\n",
    "if USE_SPECIFIC_FILE:\n",
    "    # Specific file mode\n",
    "    if target_db_file and target_db_file.exists():\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Specific File Mode\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nUsing configured database file:\")\n",
    "        print(f\"  File: {target_db_file.name}\")\n",
    "        print(f\"  Farm: {FARM_NAME}\")\n",
    "        print(f\"  Coordinate: {COORDINATE}\")\n",
    "        print(f\"  Full Path: {target_db_file}\\n\")\n",
    "        \n",
    "        # Create structure similar to discovery mode\n",
    "        location_key = f\"{FARM_NAME}/{COORDINATE}\"\n",
    "        db_files_by_location[location_key] = {\n",
    "            'farm_name': FARM_NAME,\n",
    "            'coordinate': COORDINATE,\n",
    "            'folder_path': str(target_db_file.parent),\n",
    "            'db_files': [target_db_file]\n",
    "        }\n",
    "        \n",
    "        all_db_files.append({\n",
    "            'file_path': target_db_file,\n",
    "            'farm_name': FARM_NAME,\n",
    "            'coordinate': COORDINATE,\n",
    "            'folder_path': str(target_db_file.parent),\n",
    "            'file_name': target_db_file.name\n",
    "        })\n",
    "        \n",
    "        total_files = 1\n",
    "        print(f\"‚úì Database file loaded successfully\")\n",
    "    else:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Error: Specific File Mode\")\n",
    "        print(\"=\" * 80)\n",
    "        if target_db_file:\n",
    "            print(f\"\\n‚úó Database file not found: {target_db_file}\")\n",
    "        else:\n",
    "            print(f\"\\n‚úó Invalid path configuration\")\n",
    "        print(f\"\\nPlease check:\")\n",
    "        print(f\"  - Base directory: {base_directory} (exists: {base_directory.exists()})\")\n",
    "        print(f\"  - Farm name: {FARM_NAME}\")\n",
    "        print(f\"  - Coordinate: {COORDINATE}\")\n",
    "        print(f\"  - Database file name: {DB_FILE_NAME}\")\n",
    "        \n",
    "else:\n",
    "    # Discovery mode - find all .db files\n",
    "    if base_directory.exists():\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Discovery Mode - Finding All Database Files\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Iterate through farm name folders\n",
    "        for farm_folder in base_directory.iterdir():\n",
    "            if not farm_folder.is_dir():\n",
    "                continue\n",
    "            \n",
    "            farm_name = farm_folder.name\n",
    "            \n",
    "            # Iterate through coordinate_APSIM folders within each farm\n",
    "            for coord_apsim_folder in farm_folder.iterdir():\n",
    "                if not coord_apsim_folder.is_dir():\n",
    "                    continue\n",
    "                \n",
    "                folder_name = coord_apsim_folder.name\n",
    "                \n",
    "                # Check if folder name ends with \"_APSIM\"\n",
    "                if folder_name.endswith(\"_APSIM\"):\n",
    "                    # Extract coordinate from folder name (remove \"_APSIM\" suffix)\n",
    "                    coordinate = folder_name[:-6]  # Remove \"_APSIM\" (6 characters)\n",
    "                    \n",
    "                    # Find all .db files directly in this folder\n",
    "                    db_files = list(coord_apsim_folder.glob(\"*.db\"))\n",
    "                    \n",
    "                    if db_files:\n",
    "                        location_key = f\"{farm_name}/{coordinate}\"\n",
    "                        db_files_by_location[location_key] = {\n",
    "                            'farm_name': farm_name,\n",
    "                            'coordinate': coordinate,\n",
    "                            'folder_path': str(coord_apsim_folder),\n",
    "                            'db_files': db_files\n",
    "                        }\n",
    "                        total_files += len(db_files)\n",
    "        \n",
    "        # Display results\n",
    "        if total_files > 0:\n",
    "            print(f\"\\nFound {total_files} database file(s) across {len(db_files_by_location)} location(s):\\n\")\n",
    "            \n",
    "            for location_key, location_data in db_files_by_location.items():\n",
    "                print(f\"üìç {location_key}\")\n",
    "                print(f\"   Folder: {location_data['folder_path']}\")\n",
    "                print(f\"   Files ({len(location_data['db_files'])}):\")\n",
    "                for db_file in location_data['db_files']:\n",
    "                    print(f\"     - {db_file.name}\")\n",
    "                print()\n",
    "            \n",
    "            # Create a flat list of all db files with metadata\n",
    "            for location_data in db_files_by_location.values():\n",
    "                for db_file in location_data['db_files']:\n",
    "                    all_db_files.append({\n",
    "                        'file_path': db_file,\n",
    "                        'farm_name': location_data['farm_name'],\n",
    "                        'coordinate': location_data['coordinate'],\n",
    "                        'folder_path': location_data['folder_path'],\n",
    "                        'file_name': db_file.name\n",
    "                    })\n",
    "            \n",
    "            print(f\"Total: {total_files} database file(s) found\")\n",
    "        else:\n",
    "            print(f\"\\nNo database files found in the structure.\")\n",
    "            print(f\"Checked: {base_directory}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Error: Base Directory Not Found\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\n‚úó Base directory not found: {base_directory}\")\n",
    "        print(f\"Please check the BASE_DIR configuration.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected Database File\n",
    "\n",
    "Access the configured or selected database file for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No database file available. Please check your configuration.\n"
     ]
    }
   ],
   "source": [
    "# Selected database file for processing\n",
    "# In specific file mode, this is automatically set to the configured file\n",
    "# In discovery mode, you can select a file from all_db_files or set it manually\n",
    "\n",
    "if USE_SPECIFIC_FILE and target_db_file:\n",
    "    selected_db_file = target_db_file\n",
    "    print(f\"Selected file (from configuration): {selected_db_file.name}\")\n",
    "    print(f\"Path: {selected_db_file}\")\n",
    "elif all_db_files:\n",
    "    # Default to first file found, but you can change this\n",
    "    selected_db_file = all_db_files[0]['file_path']\n",
    "    print(f\"Selected file (first from discovery): {selected_db_file.name}\")\n",
    "    print(f\"Farm: {all_db_files[0]['farm_name']}, Coordinate: {all_db_files[0]['coordinate']}\")\n",
    "    print(f\"Path: {selected_db_file}\")\n",
    "    print(f\"\\nTo select a different file, set:\")\n",
    "    print(f\"  selected_db_file = all_db_files[N]['file_path']  # where N is the index\")\n",
    "    print(f\"\\nAvailable files:\")\n",
    "    for i, file_info in enumerate(all_db_files):\n",
    "        marker = \" <-- SELECTED\" if file_info['file_path'] == selected_db_file else \"\"\n",
    "        print(f\"  [{i}] {file_info['file_name']} ({file_info['farm_name']}/{file_info['coordinate']}){marker}\")\n",
    "else:\n",
    "    selected_db_file = None\n",
    "    print(\"No database file available. Please check your configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No database files found.\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "if db_files_by_location:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Summary by Location\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Group by farm\n",
    "    farms = {}\n",
    "    for location_key, location_data in db_files_by_location.items():\n",
    "        farm_name = location_data['farm_name']\n",
    "        if farm_name not in farms:\n",
    "            farms[farm_name] = {\n",
    "                'coordinates': [],\n",
    "                'total_files': 0\n",
    "            }\n",
    "        farms[farm_name]['coordinates'].append(location_data['coordinate'])\n",
    "        farms[farm_name]['total_files'] += len(location_data['db_files'])\n",
    "    \n",
    "    print(f\"\\nTotal Farms: {len(farms)}\")\n",
    "    print(f\"Total Coordinates: {len(db_files_by_location)}\")\n",
    "    print(f\"Total Database Files: {total_files}\\n\")\n",
    "    \n",
    "    print(\"Breakdown by Farm:\")\n",
    "    print(\"-\" * 80)\n",
    "    for farm_name, farm_data in sorted(farms.items()):\n",
    "        print(f\"\\n{farm_name}:\")\n",
    "        print(f\"  Coordinates: {len(farm_data['coordinates'])}\")\n",
    "        print(f\"  Database Files: {farm_data['total_files']}\")\n",
    "        print(f\"  Coordinate List: {', '.join(sorted(farm_data['coordinates']))}\")\n",
    "else:\n",
    "    print(\"No database files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Database Files\n",
    "\n",
    "Use the `all_db_files` list to access all database files with their metadata, or iterate through `db_files_by_location` to process by location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Inspection Functions\n",
    "\n",
    "Helper functions to inspect database structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Database inspection functions loaded\n"
     ]
    }
   ],
   "source": [
    "def inspect_database(db_path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Inspect an APSIM database file and return information about its structure.\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to the SQLite database file\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing database metadata and table information\n",
    "    \"\"\"\n",
    "    if not db_path.exists():\n",
    "        return {\"error\": f\"Database file not found: {db_path}\"}\n",
    "    \n",
    "    conn = sqlite3.connect(str(db_path))\n",
    "    conn.row_factory = sqlite3.Row\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get all tables\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT name FROM sqlite_master \n",
    "        WHERE type='table' \n",
    "        ORDER BY name\n",
    "    \"\"\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "    db_info = {\n",
    "        \"database_file\": str(db_path),\n",
    "        \"tables\": {},\n",
    "        \"has_report\": \"Report\" in tables,\n",
    "        \"has_daily\": \"Daily\" in tables\n",
    "    }\n",
    "    \n",
    "    # Get schema and row counts for each table\n",
    "    for table_name in tables:\n",
    "        cursor.execute(f\"SELECT COUNT(*) as count FROM {table_name}\")\n",
    "        row_count = cursor.fetchone()[\"count\"]\n",
    "        \n",
    "        # Get column information\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "        columns = []\n",
    "        for col in cursor.fetchall():\n",
    "            columns.append({\n",
    "                \"name\": col[1],\n",
    "                \"type\": col[2],\n",
    "                \"not_null\": bool(col[3]),\n",
    "                \"default\": col[4],\n",
    "                \"primary_key\": bool(col[5])\n",
    "            })\n",
    "        \n",
    "        db_info[\"tables\"][table_name] = {\n",
    "            \"row_count\": row_count,\n",
    "            \"columns\": columns,\n",
    "            \"column_names\": [col[\"name\"] for col in columns]\n",
    "        }\n",
    "    \n",
    "    conn.close()\n",
    "    return db_info\n",
    "\n",
    "\n",
    "def print_database_summary(db_info: Dict[str, Any]) -> None:\n",
    "    \"\"\"Print a formatted summary of database information.\"\"\"\n",
    "    if \"error\" in db_info:\n",
    "        print(f\"Error: {db_info['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Database: {Path(db_info['database_file']).name}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nTables found: {len(db_info['tables'])}\")\n",
    "    print(f\"Has Report table: {db_info['has_report']}\")\n",
    "    print(f\"Has Daily table: {db_info['has_daily']}\\n\")\n",
    "    \n",
    "    for table_name, table_data in db_info[\"tables\"].items():\n",
    "        print(f\"üìä {table_name}\")\n",
    "        print(f\"   Rows: {table_data['row_count']:,}\")\n",
    "        print(f\"   Columns ({len(table_data['columns'])}): {', '.join(table_data['column_names'][:10])}\")\n",
    "        if len(table_data['column_names']) > 10:\n",
    "            print(f\"   ... and {len(table_data['column_names']) - 10} more\")\n",
    "        print()\n",
    "\n",
    "\n",
    "print(\"‚úì Database inspection functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Selected Database File\n",
    "\n",
    "Inspect the currently selected database file (from configuration or selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No database file selected or file not found.\n",
      "Please check your configuration or select a file from all_db_files.\n"
     ]
    }
   ],
   "source": [
    "# Inspect the selected database file\n",
    "if 'selected_db_file' in globals() and selected_db_file and selected_db_file.exists():\n",
    "    # Find file info if available\n",
    "    file_info = None\n",
    "    for info in all_db_files:\n",
    "        if info['file_path'] == selected_db_file:\n",
    "            file_info = info\n",
    "            break\n",
    "    \n",
    "    if file_info:\n",
    "        print(f\"Inspecting: {selected_db_file.name}\")\n",
    "        print(f\"Location: {file_info['farm_name']}/{file_info['coordinate']}\\n\")\n",
    "    else:\n",
    "        print(f\"Inspecting: {selected_db_file.name}\\n\")\n",
    "    \n",
    "    db_info = inspect_database(selected_db_file)\n",
    "    print_database_summary(db_info)\n",
    "    \n",
    "    # Check if Report and Daily tables have expected columns\n",
    "    if db_info['has_report']:\n",
    "        report_cols = db_info['tables']['Report']['column_names']\n",
    "        # Note: Report table uses \"Yield\" not \"DryYield\"\n",
    "        expected_report_cols = ['Year', 'Yield', 'SowingDate', 'HarvestDate', 'CropSurvived']\n",
    "        missing_cols = [col for col in expected_report_cols if col not in report_cols]\n",
    "        if missing_cols:\n",
    "            print(f\"‚ö† Note: Some expected Report columns not found: {missing_cols}\")\n",
    "        else:\n",
    "            print(\"‚úì Report table contains expected columns\")\n",
    "    \n",
    "    if db_info['has_daily']:\n",
    "        daily_cols = db_info['tables']['Daily']['column_names']\n",
    "        # Note: Daily table uses \"RadiationsFactor\" (with 's') not \"RadiationFactor\"\n",
    "        expected_daily_cols = ['Date', 'Year', 'DryYield', 'RadiationsFactor', 'WaterFactor']\n",
    "        missing_cols = [col for col in expected_daily_cols if col not in daily_cols]\n",
    "        if missing_cols:\n",
    "            print(f\"‚ö† Note: Some expected Daily columns not found: {missing_cols}\")\n",
    "        else:\n",
    "            print(\"‚úì Daily table contains expected columns\")\n",
    "else:\n",
    "    print(\"No database file selected or file not found.\")\n",
    "    print(\"Please check your configuration or select a file from all_db_files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database to CSV/Excel Conversion\n",
    "\n",
    "Convert the selected database file to CSV/Excel format. Creates an Excel file (.xlsx) with multiple sheets (one per table), or individual CSV files for each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Database conversion functions loaded\n"
     ]
    }
   ],
   "source": [
    "def convert_db_to_excel(db_path: Path, output_path: Path = None, include_empty_tables: bool = False) -> Path:\n",
    "    \"\"\"\n",
    "    Convert an APSIM SQLite database to an Excel file with multiple sheets.\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to the SQLite database file\n",
    "        output_path: Path for the output Excel file (default: same folder as db file, with .xlsx extension)\n",
    "        include_empty_tables: Whether to include tables with 0 rows\n",
    "    \n",
    "    Returns:\n",
    "        Path to the created Excel file\n",
    "    \"\"\"\n",
    "    if not db_path.exists():\n",
    "        raise FileNotFoundError(f\"Database file not found: {db_path}\")\n",
    "    \n",
    "    # Set default output path if not provided\n",
    "    if output_path is None:\n",
    "        output_path = db_path.parent / f\"{db_path.stem}.xlsx\"\n",
    "    \n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect(str(db_path))\n",
    "    \n",
    "    # Get all tables\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "    print(f\"Converting database: {db_path.name}\")\n",
    "    print(f\"Output file: {output_path.name}\")\n",
    "    print(f\"Found {len(tables)} table(s)\\n\")\n",
    "    \n",
    "    # Create Excel writer\n",
    "    with pd.ExcelWriter(str(output_path), engine='openpyxl') as writer:\n",
    "        exported_tables = 0\n",
    "        \n",
    "        for table_name in tables:\n",
    "            # Get row count\n",
    "            cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "            row_count = cursor.fetchone()[0]\n",
    "            \n",
    "            if row_count == 0 and not include_empty_tables:\n",
    "                print(f\"  ‚è≠ Skipping {table_name} (empty table)\")\n",
    "                continue\n",
    "            \n",
    "            # Read table into DataFrame\n",
    "            try:\n",
    "                df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "                \n",
    "                # Clean sheet name (Excel has limitations on sheet names)\n",
    "                sheet_name = table_name[:31]  # Excel sheet name limit is 31 characters\n",
    "                sheet_name = sheet_name.replace('/', '_').replace('\\\\', '_').replace('?', '_').replace('*', '_')\n",
    "                sheet_name = sheet_name.replace('[', '').replace(']', '')\n",
    "                \n",
    "                # Write to Excel sheet\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                exported_tables += 1\n",
    "                print(f\"  ‚úì Exported {table_name} ({row_count:,} rows) -> Sheet: {sheet_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚úó Error exporting {table_name}: {e}\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"\\n‚úì Conversion complete!\")\n",
    "    print(f\"  Exported {exported_tables} table(s) to {output_path.name}\")\n",
    "    print(f\"  File location: {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n",
    "def convert_db_to_csv_files(db_path: Path, output_dir: Path = None, include_empty_tables: bool = False) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Convert an APSIM SQLite database to multiple CSV files (one per table).\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to the SQLite database file\n",
    "        output_dir: Directory for CSV files (default: creates a folder named after the db file in the same directory)\n",
    "        include_empty_tables: Whether to include tables with 0 rows\n",
    "    \n",
    "    Returns:\n",
    "        List of paths to created CSV files\n",
    "    \"\"\"\n",
    "    if not db_path.exists():\n",
    "        raise FileNotFoundError(f\"Database file not found: {db_path}\")\n",
    "    \n",
    "    # Set default output directory if not provided\n",
    "    if output_dir is None:\n",
    "        # Create a folder named after the database file (without .db extension)\n",
    "        output_dir = db_path.parent / db_path.stem\n",
    "    \n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect(str(db_path))\n",
    "    \n",
    "    # Get all tables\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "    print(f\"Converting database: {db_path.name}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(f\"Found {len(tables)} table(s)\\n\")\n",
    "    \n",
    "    csv_files = []\n",
    "    \n",
    "    for table_name in tables:\n",
    "        # Skip metadata tables (tables starting with underscore), except _InitialConditions\n",
    "        # This skips: _Checkpoints, _Simulations, _Messages, _Units, etc.\n",
    "        # Only _InitialConditions is included (and renamed to Parameters)\n",
    "        if table_name.startswith('_') and table_name != '_InitialConditions':\n",
    "            print(f\"  ‚è≠ Skipping {table_name} (metadata table)\")\n",
    "            continue\n",
    "        \n",
    "        # Get row count\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        row_count = cursor.fetchone()[0]\n",
    "        \n",
    "        if row_count == 0 and not include_empty_tables:\n",
    "            print(f\"  ‚è≠ Skipping {table_name} (empty table)\")\n",
    "            continue\n",
    "        \n",
    "        # Read table into DataFrame\n",
    "        try:\n",
    "            df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "            \n",
    "            # Format Date and SowingDate columns for Daily table (remove time portion)\n",
    "            if table_name == 'Daily':\n",
    "                try:\n",
    "                    # Convert Date to datetime if it exists (date-only, no time component)\n",
    "                    if 'Date' in df.columns:\n",
    "                        try:\n",
    "                            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "                            # Normalize to remove time component (keeps date-only)\n",
    "                            df['Date'] = df['Date'].dt.normalize()\n",
    "                        except Exception as e:\n",
    "                            print(f\"    ‚ö† Warning: Could not format Date column: {e}\")\n",
    "                    \n",
    "                    # Convert SowingDate to datetime if it exists (date-only, no time component)\n",
    "                    if 'SowingDate' in df.columns:\n",
    "                        try:\n",
    "                            df['SowingDate'] = pd.to_datetime(df['SowingDate'], errors='coerce')\n",
    "                            # Normalize to remove time component (keeps date-only)\n",
    "                            df['SowingDate'] = df['SowingDate'].dt.normalize()\n",
    "                        except Exception as e:\n",
    "                            print(f\"    ‚ö† Warning: Could not format SowingDate column: {e}\")\n",
    "                    \n",
    "                    # Fix Year column if it's 0 or missing - extract year from Date column\n",
    "                    if 'Year' in df.columns and 'Date' in df.columns:\n",
    "                        try:\n",
    "                            # Extract year from Date column where Year is 0, missing, or invalid\n",
    "                            # Only proceed if Date column has valid datetime values\n",
    "                            if df['Date'].notna().any():\n",
    "                                date_years = df['Date'].dt.year\n",
    "                                # Update Year column where it's 0, NaN, or less than 1900, and Date is valid\n",
    "                                mask = ((df['Year'] == 0) | (df['Year'].isna()) | (df['Year'] < 1900)) & df['Date'].notna()\n",
    "                                df.loc[mask, 'Year'] = date_years[mask]\n",
    "                        except Exception as e:\n",
    "                            print(f\"    ‚ö† Warning: Could not fix Year column: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    ‚ö† Warning: Error in date formatting for {table_name}: {e}\")\n",
    "            \n",
    "            # Create CSV filename (rename _InitialConditions to Parameters)\n",
    "            display_name = \"Parameters\" if table_name == \"_InitialConditions\" else table_name\n",
    "            csv_filename = f\"{db_path.stem}_{display_name}.csv\"\n",
    "            csv_path = output_dir / csv_filename\n",
    "            \n",
    "            # Write to CSV\n",
    "            df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "            csv_files.append(csv_path)\n",
    "            print(f\"  ‚úì Exported {display_name} ({row_count:,} rows) -> {csv_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error exporting {table_name}: {e}\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"\\n‚úì Conversion complete!\")\n",
    "    print(f\"  Exported {len(csv_files)} CSV file(s) to {output_dir}\")\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "\n",
    "print(\"‚úì Database conversion functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Selected Database to CSV\n",
    "\n",
    "Convert the selected database file to CSV format. Creates multiple `.csv` files, one file per table.\n",
    "\n",
    "**Output Location**: Files are saved in a **folder named after the database file** (without the `.db` extension), created in the same directory as the database file.\n",
    "\n",
    "**Note**: Requires `pandas` package. Install with:\n",
    "```bash\n",
    "pip install pandas\n",
    "```\n",
    "\n",
    "**CSV Format**:\n",
    "- Creates multiple `.csv` files, one file per table\n",
    "- File names: `{database_name}_{table_name}.csv`\n",
    "- Location: A folder named `{database_name}` (without `.db`) in the same directory as the database file\n",
    "- Example: For `ClimAdapt_Wheat_neg31.45_117.55_past.db`, CSV files are saved in `ClimAdapt_Wheat_neg31.45_117.55_past/` folder:\n",
    "  - `ClimAdapt_Wheat_neg31.45_117.55_past/ClimAdapt_Wheat_neg31.45_117.55_past_Daily.csv`\n",
    "  - `ClimAdapt_Wheat_neg31.45_117.55_past/ClimAdapt_Wheat_neg31.45_117.55_past_Report.csv`\n",
    "  - etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No database file selected or file not found.\n",
      "Please check your configuration or select a file from all_db_files.\n"
     ]
    }
   ],
   "source": [
    "# Conversion configuration\n",
    "INCLUDE_EMPTY_TABLES = False  # Set to True to include tables with 0 rows\n",
    "\n",
    "# Convert the selected database file to CSV format\n",
    "# Output files will be saved in a folder named after the database file (without .db extension)\n",
    "if 'selected_db_file' in globals() and selected_db_file and selected_db_file.exists():\n",
    "    try:\n",
    "        # Output folder will be created: {database_file_name}/ (without .db extension)\n",
    "        output_folder = selected_db_file.parent / selected_db_file.stem\n",
    "        \n",
    "        # Convert to multiple CSV files\n",
    "        # Output will be saved in a folder named after the database file\n",
    "        csv_files = convert_db_to_csv_files(\n",
    "            selected_db_file,\n",
    "            output_dir=None,  # None = create folder named after db file\n",
    "            include_empty_tables=INCLUDE_EMPTY_TABLES\n",
    "        )\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"‚úì CSV files created successfully!\")\n",
    "        print(f\"  Created {len(csv_files)} CSV file(s):\")\n",
    "        for csv_file in csv_files:\n",
    "            print(f\"    - {csv_file.name}\")\n",
    "        print(f\"  Saved in: {output_folder}\")\n",
    "        print(f\"{'='*80}\")\n",
    "            \n",
    "    except ImportError as e:\n",
    "        print(f\"Error: Missing required package. {e}\")\n",
    "        print(\"\\nPlease install required package:\")\n",
    "        print(\"  pip install pandas\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during conversion: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"No database file selected or file not found.\")\n",
    "    print(\"Please check your configuration or select a file from all_db_files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
