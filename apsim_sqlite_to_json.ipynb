{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# APSIM SQLite to LLM-ready JSON Converter\n",
        "\n",
        "Converts SQLite databases generated by APSIM (Agricultural Production Systems sIMulator) or APSIM-derived pipelines into clean, deterministic, LLM-readable JSON format while preserving scientific meaning.\n",
        "\n",
        "## Features\n",
        "\n",
        "- **Complete table enumeration**: Automatically discovers and exports all tables in the database\n",
        "- **APSIM-aware processing**: Special handling for Daily, Report, _Messages, and metadata tables\n",
        "- **Data cleaning**: Converts dates to ISO format, handles APSIM placeholder dates, and preserves data types\n",
        "- **LLM-ready output**: Structured JSON format optimized for AI reasoning and analysis\n",
        "- **Data dictionary**: Auto-generates APSIM-aware column descriptions\n",
        "- **Year-grouped exports**: For large Daily tables (>5000 rows), creates additional year-grouped files\n",
        "\n",
        "## Default Settings\n",
        "\n",
        "- **Input directory**: `C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files`\n",
        "- **Output location**: Same folder as input file (not a subdirectory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Imports successful\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import sqlite3\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from collections import defaultdict\n",
        "\n",
        "# Default input directory\n",
        "DEFAULT_INPUT_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\"\n",
        "\n",
        "print(\"✓ Imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## APSIMSQLiteConverter Class\n",
        "\n",
        "The main converter class that handles all database operations and JSON export."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ APSIMSQLiteConverter class loaded\n"
          ]
        }
      ],
      "source": [
        "class APSIMSQLiteConverter:\n",
        "    \"\"\"Converts APSIM SQLite databases to LLM-ready JSON format.\"\"\"\n",
        "    \n",
        "    # APSIM placeholder dates that should be converted to null\n",
        "    INVALID_DATES = [\n",
        "        '0001-01-01',\n",
        "        '0001-01-01 00:00:00',\n",
        "        '0001-01-01T00:00:00',\n",
        "        '0001-01-01T00:00:00Z'\n",
        "    ]\n",
        "    \n",
        "    def __init__(self, db_path: str, output_dir: str = None):\n",
        "        \"\"\"\n",
        "        Initialize converter.\n",
        "        \n",
        "        Args:\n",
        "            db_path: Path to SQLite database file\n",
        "            output_dir: Directory to write JSON files (default: same folder as input file)\n",
        "        \"\"\"\n",
        "        self.db_path = Path(db_path)\n",
        "        if not self.db_path.exists():\n",
        "            raise FileNotFoundError(f\"Database file not found: {db_path}\")\n",
        "        \n",
        "        if output_dir is None:\n",
        "            # Write to same folder as input file\n",
        "            self.output_dir = self.db_path.parent\n",
        "        else:\n",
        "            self.output_dir = Path(output_dir)\n",
        "        \n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.conn = None\n",
        "        self.data_dictionary = {}\n",
        "    \n",
        "    def __enter__(self):\n",
        "        \"\"\"Context manager entry.\"\"\"\n",
        "        self.conn = sqlite3.connect(str(self.db_path))\n",
        "        self.conn.row_factory = sqlite3.Row  # Enable column access by name\n",
        "        return self\n",
        "    \n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        \"\"\"Context manager exit.\"\"\"\n",
        "        if self.conn:\n",
        "            self.conn.close()\n",
        "    \n",
        "    def get_tables(self) -> List[str]:\n",
        "        \"\"\"Enumerate all tables in the database.\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        cursor.execute(\"\"\"\n",
        "            SELECT name FROM sqlite_master \n",
        "            WHERE type='table' \n",
        "            ORDER BY name\n",
        "        \"\"\")\n",
        "        return [row[0] for row in cursor.fetchall()]\n",
        "    \n",
        "    def get_table_schema(self, table_name: str) -> Dict[str, str]:\n",
        "        \"\"\"Get column names and types for a table.\"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
        "        schema = {}\n",
        "        for row in cursor.fetchall():\n",
        "            col_name = row[1]\n",
        "            col_type = row[2].upper()\n",
        "            # Normalize SQLite types\n",
        "            if 'INT' in col_type:\n",
        "                schema[col_name] = 'integer'\n",
        "            elif 'REAL' in col_type or 'FLOAT' in col_type or 'DOUBLE' in col_type:\n",
        "                schema[col_name] = 'float'\n",
        "            elif 'TEXT' in col_type or 'CHAR' in col_type or 'VARCHAR' in col_type:\n",
        "                schema[col_name] = 'text'\n",
        "            elif 'BLOB' in col_type:\n",
        "                schema[col_name] = 'blob'\n",
        "            else:\n",
        "                schema[col_name] = 'text'  # Default\n",
        "        return schema\n",
        "    \n",
        "    def clean_value(self, value: Any, col_type: str) -> Any:\n",
        "        \"\"\"\n",
        "        Clean a single value according to APSIM rules.\n",
        "        \n",
        "        Args:\n",
        "            value: Raw value from database\n",
        "            col_type: Column type from schema\n",
        "        \n",
        "        Returns:\n",
        "            Cleaned value\n",
        "        \"\"\"\n",
        "        # Handle None\n",
        "        if value is None:\n",
        "            return None\n",
        "        \n",
        "        # Handle empty strings\n",
        "        if isinstance(value, str) and value.strip() == '':\n",
        "            return None\n",
        "        \n",
        "        # Handle dates\n",
        "        if isinstance(value, str):\n",
        "            # Check for invalid APSIM placeholder dates\n",
        "            if value.strip() in self.INVALID_DATES:\n",
        "                return None\n",
        "            \n",
        "            # Try to parse and reformat dates\n",
        "            if self._looks_like_date(value):\n",
        "                cleaned = self._clean_date(value)\n",
        "                if cleaned:\n",
        "                    return cleaned\n",
        "        \n",
        "        # Preserve types\n",
        "        if col_type == 'integer':\n",
        "            try:\n",
        "                return int(value)\n",
        "            except (ValueError, TypeError):\n",
        "                return value\n",
        "        elif col_type == 'float':\n",
        "            try:\n",
        "                return float(value)\n",
        "            except (ValueError, TypeError):\n",
        "                return value\n",
        "        elif col_type == 'text':\n",
        "            return str(value)\n",
        "        \n",
        "        return value\n",
        "    \n",
        "    def _looks_like_date(self, value: str) -> bool:\n",
        "        \"\"\"Check if a string looks like a date.\"\"\"\n",
        "        if not isinstance(value, str):\n",
        "            return False\n",
        "        \n",
        "        # Common date patterns\n",
        "        date_patterns = [\n",
        "            r'\\d{4}-\\d{2}-\\d{2}',  # YYYY-MM-DD\n",
        "            r'\\d{2}/\\d{2}/\\d{4}',  # MM/DD/YYYY\n",
        "            r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}',  # YYYY-MM-DD HH:MM:SS\n",
        "        ]\n",
        "        \n",
        "        for pattern in date_patterns:\n",
        "            if re.match(pattern, value.strip()):\n",
        "                return True\n",
        "        \n",
        "        return False\n",
        "    \n",
        "    def _clean_date(self, value: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Convert date string to ISO format YYYY-MM-DD.\n",
        "        \n",
        "        Returns None if date is invalid or placeholder.\n",
        "        \"\"\"\n",
        "        if not value or value.strip() in self.INVALID_DATES:\n",
        "            return None\n",
        "        \n",
        "        value = value.strip()\n",
        "        \n",
        "        # Already in ISO format?\n",
        "        if len(value) >= 10 and value[4] == '-' and value[7] == '-':\n",
        "            date_part = value[:10]\n",
        "            if date_part in self.INVALID_DATES:\n",
        "                return None\n",
        "            return date_part\n",
        "        \n",
        "        # Try parsing various formats\n",
        "        date_formats = [\n",
        "            '%Y-%m-%d',\n",
        "            '%Y-%m-%d %H:%M:%S',\n",
        "            '%Y-%m-%dT%H:%M:%S',\n",
        "            '%Y-%m-%dT%H:%M:%SZ',\n",
        "            '%m/%d/%Y',\n",
        "            '%d/%m/%Y',\n",
        "            '%Y/%m/%d',\n",
        "        ]\n",
        "        \n",
        "        for fmt in date_formats:\n",
        "            try:\n",
        "                dt = datetime.strptime(value, fmt)\n",
        "                return dt.strftime('%Y-%m-%d')\n",
        "            except ValueError:\n",
        "                continue\n",
        "        \n",
        "        # If we can't parse it, return as-is (might not be a date)\n",
        "        return value\n",
        "    \n",
        "    def export_table(self, table_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Export a single table to JSON structure.\n",
        "        \n",
        "        Returns:\n",
        "            Dictionary with table data in the required JSON structure\n",
        "        \"\"\"\n",
        "        cursor = self.conn.cursor()\n",
        "        cursor.execute(f\"SELECT * FROM {table_name}\")\n",
        "        \n",
        "        schema = self.get_table_schema(table_name)\n",
        "        rows = cursor.fetchall()\n",
        "        \n",
        "        # Clean rows\n",
        "        cleaned_rows = []\n",
        "        for row in rows:\n",
        "            cleaned_row = {}\n",
        "            for col_name, col_type in schema.items():\n",
        "                value = row[col_name]\n",
        "                cleaned_value = self.clean_value(value, col_type)\n",
        "                cleaned_row[col_name] = cleaned_value\n",
        "            cleaned_rows.append(cleaned_row)\n",
        "        \n",
        "        return {\n",
        "            \"table\": table_name,\n",
        "            \"row_count\": len(cleaned_rows),\n",
        "            \"columns\": schema,\n",
        "            \"rows\": cleaned_rows\n",
        "        }\n",
        "    \n",
        "    def export_daily_table(self, table_name: str) -> None:\n",
        "        \"\"\"\n",
        "        Export Daily table with APSIM-specific handling.\n",
        "        \n",
        "        If row_count > 5000, also generates Daily_by_year.json.\n",
        "        \"\"\"\n",
        "        data = self.export_table(table_name)\n",
        "        \n",
        "        # Write main Daily.json\n",
        "        output_path = self.output_dir / f\"{table_name}.json\"\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"  ✓ Exported {table_name}.json ({data['row_count']} rows)\")\n",
        "        \n",
        "        # Generate Daily_by_year.json if row_count > 5000\n",
        "        if data['row_count'] > 5000:\n",
        "            grouped_data = self._group_daily_by_year(data)\n",
        "            output_path = self.output_dir / f\"{table_name}_by_year.json\"\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(grouped_data, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"  ✓ Exported {table_name}_by_year.json\")\n",
        "    \n",
        "    def _group_daily_by_year(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Group Daily table rows by Year column.\"\"\"\n",
        "        grouped = defaultdict(list)\n",
        "        \n",
        "        for row in data['rows']:\n",
        "            year = row.get('Year')\n",
        "            if year is not None:\n",
        "                grouped[str(year)].append(row)\n",
        "            else:\n",
        "                grouped['null'].append(row)\n",
        "        \n",
        "        return {\n",
        "            \"table\": data['table'],\n",
        "            \"grouped_by\": \"Year\",\n",
        "            \"data\": dict(grouped)\n",
        "        }\n",
        "    \n",
        "    def export_report_table(self, table_name: str) -> None:\n",
        "        \"\"\"Export Report table with APSIM-specific handling.\"\"\"\n",
        "        data = self.export_table(table_name)\n",
        "        \n",
        "        # Ensure date fields are cleaned\n",
        "        date_columns = [col for col in data['columns'].keys() \n",
        "                       if any(keyword in col.lower() for keyword in ['date', 'sowing', 'flowering', 'maturity'])]\n",
        "        \n",
        "        for row in data['rows']:\n",
        "            for col in date_columns:\n",
        "                if col in row:\n",
        "                    row[col] = self._clean_date(str(row[col])) if row[col] else None\n",
        "        \n",
        "        output_path = self.output_dir / f\"{table_name}.json\"\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"  ✓ Exported {table_name}.json ({data['row_count']} rows)\")\n",
        "    \n",
        "    def export_messages_table(self, table_name: str) -> None:\n",
        "        \"\"\"Export _Messages table preserving verbatim message text.\"\"\"\n",
        "        data = self.export_table(table_name)\n",
        "        \n",
        "        output_path = self.output_dir / f\"{table_name}.json\"\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"  ✓ Exported {table_name}.json ({data['row_count']} rows)\")\n",
        "    \n",
        "    def export_metadata_table(self, table_name: str) -> None:\n",
        "        \"\"\"Export metadata tables (_Simulations, _Checkpoints, etc.).\"\"\"\n",
        "        data = self.export_table(table_name)\n",
        "        \n",
        "        output_path = self.output_dir / f\"{table_name}.json\"\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"  ✓ Exported {table_name}.json ({data['row_count']} rows)\")\n",
        "    \n",
        "    def generate_data_dictionary(self, tables: List[str]) -> None:\n",
        "        \"\"\"\n",
        "        Generate data_dictionary.json with APSIM-aware descriptions.\n",
        "        \n",
        "        Args:\n",
        "            tables: List of table names to document\n",
        "        \"\"\"\n",
        "        dictionary = {}\n",
        "        \n",
        "        for table_name in tables:\n",
        "            schema = self.get_table_schema(table_name)\n",
        "            dictionary[table_name] = {}\n",
        "            \n",
        "            for col_name, col_type in schema.items():\n",
        "                description = self._infer_column_description(table_name, col_name, col_type)\n",
        "                dictionary[table_name][col_name] = {\n",
        "                    \"type\": col_type,\n",
        "                    \"description\": description\n",
        "                }\n",
        "        \n",
        "        output_path = self.output_dir / \"data_dictionary.json\"\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(dictionary, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"  ✓ Generated data_dictionary.json\")\n",
        "    \n",
        "    def _infer_column_description(self, table_name: str, col_name: str, col_type: str) -> str:\n",
        "        \"\"\"\n",
        "        Infer APSIM-aware description for a column.\n",
        "        \n",
        "        Uses common APSIM terminology and patterns.\n",
        "        \"\"\"\n",
        "        col_lower = col_name.lower()\n",
        "        \n",
        "        # Phenology-related\n",
        "        if 'flowering' in col_lower and 'day' in col_lower:\n",
        "            return \"Number of days from sowing to flowering as simulated by APSIM phenology\"\n",
        "        if 'maturity' in col_lower and 'day' in col_lower:\n",
        "            return \"Number of days from sowing to maturity as simulated by APSIM phenology\"\n",
        "        if 'sowing' in col_lower and 'day' in col_lower:\n",
        "            return \"Number of days from start of simulation to sowing\"\n",
        "        \n",
        "        # Dates\n",
        "        if 'date' in col_lower:\n",
        "            if 'sowing' in col_lower:\n",
        "                return \"Sowing date in ISO format (YYYY-MM-DD)\"\n",
        "            if 'flowering' in col_lower:\n",
        "                return \"Flowering date in ISO format (YYYY-MM-DD)\"\n",
        "            if 'maturity' in col_lower:\n",
        "                return \"Maturity/harvest date in ISO format (YYYY-MM-DD)\"\n",
        "            return \"Date in ISO format (YYYY-MM-DD)\"\n",
        "        \n",
        "        # Yield and biomass\n",
        "        if 'yield' in col_lower:\n",
        "            return \"Crop yield as simulated by APSIM (units depend on crop and configuration)\"\n",
        "        if 'biomass' in col_lower:\n",
        "            return \"Plant biomass pool as simulated by APSIM\"\n",
        "        if 'drymatter' in col_lower or 'dm' in col_lower:\n",
        "            return \"Dry matter content as simulated by APSIM\"\n",
        "        \n",
        "        # Water balance\n",
        "        if 'water' in col_lower or 'sw' in col_lower:\n",
        "            if 'stress' in col_lower:\n",
        "                return \"Water stress index (0-1, where 1 indicates no stress)\"\n",
        "            if 'content' in col_lower or 'mm' in col_lower:\n",
        "                return \"Soil water content (typically in mm)\"\n",
        "            return \"Water-related variable from APSIM water balance module\"\n",
        "        \n",
        "        # Nitrogen\n",
        "        if 'nitrogen' in col_lower or 'n_' in col_lower or col_lower.startswith('n '):\n",
        "            if 'stress' in col_lower:\n",
        "                return \"Nitrogen stress index (0-1, where 1 indicates no stress)\"\n",
        "            return \"Nitrogen-related variable from APSIM nitrogen balance module\"\n",
        "        \n",
        "        # Weather/climate\n",
        "        if 'rain' in col_lower or 'precip' in col_lower:\n",
        "            return \"Precipitation/rainfall (typically in mm)\"\n",
        "        if 'temp' in col_lower or 'temperature' in col_lower:\n",
        "            return \"Temperature (typically in degrees Celsius)\"\n",
        "        if 'radn' in col_lower or 'radiation' in col_lower:\n",
        "            return \"Solar radiation (typically in MJ/m²)\"\n",
        "        if 'pan' in col_lower:\n",
        "            return \"Pan evaporation (typically in mm)\"\n",
        "        \n",
        "        # Stress indices\n",
        "        if 'stress' in col_lower:\n",
        "            return \"Stress index (0-1, where 1 indicates no stress, 0 indicates maximum stress)\"\n",
        "        \n",
        "        # Survival\n",
        "        if 'survival' in col_lower or 'survive' in col_lower:\n",
        "            return \"Crop survival flag (boolean or numeric indicator)\"\n",
        "        \n",
        "        # Simulation identifiers\n",
        "        if 'simulation' in col_lower or 'sim' in col_lower:\n",
        "            return \"Simulation identifier or name\"\n",
        "        if 'zone' in col_lower:\n",
        "            return \"Geographic zone or region identifier\"\n",
        "        if 'cultivar' in col_lower or 'variety' in col_lower:\n",
        "            return \"Crop cultivar or variety identifier\"\n",
        "        if col_lower == 'year':\n",
        "            return \"Simulation year\"\n",
        "        if col_lower in ['day', 'dayofyear', 'doy']:\n",
        "            return \"Day of year (1-365/366)\"\n",
        "        \n",
        "        # Messages\n",
        "        if table_name.startswith('_Messages') or 'message' in col_lower:\n",
        "            if 'text' in col_lower or 'message' in col_lower:\n",
        "                return \"Message text from APSIM (warnings, errors, or informational)\"\n",
        "            if 'severity' in col_lower or 'type' in col_lower:\n",
        "                return \"Message severity or type (e.g., warning, error, info)\"\n",
        "        \n",
        "        # Metadata tables\n",
        "        if table_name.startswith('_'):\n",
        "            return f\"Metadata field from {table_name} table\"\n",
        "        \n",
        "        # Generic fallback\n",
        "        return f\"{col_type} field from {table_name} table (meaning may be context-dependent)\"\n",
        "    \n",
        "    def convert(self) -> None:\n",
        "        \"\"\"Main conversion method - processes entire database.\"\"\"\n",
        "        print(f\"Converting APSIM SQLite database: {self.db_path.name}\")\n",
        "        print(f\"Output directory: {self.output_dir}\")\n",
        "        print()\n",
        "        \n",
        "        tables = self.get_tables()\n",
        "        print(f\"Found {len(tables)} table(s): {', '.join(tables)}\")\n",
        "        print()\n",
        "        \n",
        "        # Process each table\n",
        "        for table_name in tables:\n",
        "            print(f\"Processing {table_name}...\")\n",
        "            \n",
        "            try:\n",
        "                if 'Daily' in table_name and not table_name.startswith('_'):\n",
        "                    self.export_daily_table(table_name)\n",
        "                elif 'Report' in table_name and not table_name.startswith('_'):\n",
        "                    self.export_report_table(table_name)\n",
        "                elif table_name.startswith('_Messages') or '_Messages' in table_name:\n",
        "                    self.export_messages_table(table_name)\n",
        "                elif table_name.startswith('_'):\n",
        "                    self.export_metadata_table(table_name)\n",
        "                else:\n",
        "                    # Generic table export\n",
        "                    data = self.export_table(table_name)\n",
        "                    output_path = self.output_dir / f\"{table_name}.json\"\n",
        "                    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                        json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "                    print(f\"  ✓ Exported {table_name}.json ({data['row_count']} rows)\")\n",
        "            \n",
        "            except Exception as e:\n",
        "                print(f\"  ✗ Error processing {table_name}: {e}\")\n",
        "                raise\n",
        "        \n",
        "        print()\n",
        "        print(\"Generating data dictionary...\")\n",
        "        self.generate_data_dictionary(tables)\n",
        "        \n",
        "        print()\n",
        "        print(f\"✓ Conversion complete! Output written to: {self.output_dir}\")\n",
        "\n",
        "print(\"✓ APSIMSQLiteConverter class loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Examples\n",
        "\n",
        "### Option 1: Convert a Single Database File\n",
        "\n",
        "Specify the path to a single `.db` file. Outputs will be written to the same folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 2: Batch Process All Databases in Default Directory\n",
        "\n",
        "Process all `.db` files in the default input directory automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6 database file(s) in C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\n",
            "\n",
            "============================================================\n",
            "Processing: ClimAdapt_Wheat_neg31.45_117.55_245_calibrated.db\n",
            "============================================================\n",
            "Converting APSIM SQLite database: ClimAdapt_Wheat_neg31.45_117.55_245_calibrated.db\n",
            "Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\n",
            "\n",
            "Found 7 table(s): Daily, Report, _Checkpoints, _InitialConditions, _Messages, _Simulations, _Units\n",
            "\n",
            "Processing Daily...\n",
            "  ✓ Exported Daily.json (10574 rows)\n",
            "  ✓ Exported Daily_by_year.json\n",
            "Processing Report...\n",
            "  ✓ Exported Report.json (27 rows)\n",
            "Processing _Checkpoints...\n",
            "  ✓ Exported _Checkpoints.json (1 rows)\n",
            "Processing _InitialConditions...\n",
            "  ✓ Exported _InitialConditions.json (198 rows)\n",
            "Processing _Messages...\n",
            "  ✓ Exported _Messages.json (1936 rows)\n",
            "Processing _Simulations...\n",
            "  ✓ Exported _Simulations.json (1 rows)\n",
            "Processing _Units...\n",
            "  ✓ Exported _Units.json (2 rows)\n",
            "\n",
            "Generating data dictionary...\n",
            "  ✓ Generated data_dictionary.json\n",
            "\n",
            "✓ Conversion complete! Output written to: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\n",
            "\n",
            "============================================================\n",
            "Processing: ClimAdapt_Wheat_neg31.45_117.55_585_calibrated.db\n",
            "============================================================\n",
            "Converting APSIM SQLite database: ClimAdapt_Wheat_neg31.45_117.55_585_calibrated.db\n",
            "Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\n",
            "\n",
            "Found 7 table(s): Daily, Report, _Checkpoints, _InitialConditions, _Messages, _Simulations, _Units\n",
            "\n",
            "Processing Daily...\n",
            "  ✓ Exported Daily.json (10574 rows)\n",
            "  ✓ Exported Daily_by_year.json\n",
            "Processing Report...\n",
            "  ✓ Exported Report.json (26 rows)\n",
            "Processing _Checkpoints...\n",
            "  ✓ Exported _Checkpoints.json (1 rows)\n",
            "Processing _InitialConditions...\n",
            "  ✓ Exported _InitialConditions.json (198 rows)\n",
            "Processing _Messages...\n",
            "  ✓ Exported _Messages.json (2175 rows)\n",
            "Processing _Simulations...\n",
            "  ✓ Exported _Simulations.json (1 rows)\n",
            "Processing _Units...\n",
            "  ✓ Exported _Units.json (2 rows)\n",
            "\n",
            "Generating data dictionary...\n",
            "  ✓ Generated data_dictionary.json\n",
            "\n",
            "✓ Conversion complete! Output written to: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\n",
            "\n",
            "============================================================\n",
            "Processing: ClimAdapt_Wheat_neg31.45_117.55_past.db\n",
            "============================================================\n",
            "Converting APSIM SQLite database: ClimAdapt_Wheat_neg31.45_117.55_past.db\n",
            "Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\n",
            "\n",
            "Found 7 table(s): Daily, Report, _Checkpoints, _InitialConditions, _Messages, _Simulations, _Units\n",
            "\n",
            "Processing Daily...\n",
            "  ✓ Exported Daily.json (9114 rows)\n",
            "  ✓ Exported Daily_by_year.json\n",
            "Processing Report...\n",
            "  ✓ Exported Report.json (24 rows)\n",
            "Processing _Checkpoints...\n",
            "  ✓ Exported _Checkpoints.json (1 rows)\n",
            "Processing _InitialConditions...\n",
            "  ✓ Exported _InitialConditions.json (198 rows)\n",
            "Processing _Messages...\n",
            "  ✓ Exported _Messages.json (2039 rows)\n",
            "Processing _Simulations...\n",
            "  ✓ Exported _Simulations.json (1 rows)\n",
            "Processing _Units...\n",
            "  ✓ Exported _Units.json (2 rows)\n",
            "\n",
            "Generating data dictionary...\n",
            "  ✓ Generated data_dictionary.json\n",
            "\n",
            "✓ Conversion complete! Output written to: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\n",
            "\n",
            "============================================================\n",
            "Processing: ClimAdapt_Wheat_neg31.75_117.60_245_calibrated.db\n",
            "============================================================\n",
            "Converting APSIM SQLite database: ClimAdapt_Wheat_neg31.75_117.60_245_calibrated.db\n",
            "Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\n",
            "\n",
            "Found 2 table(s): Daily, _InitialConditions\n",
            "\n",
            "Processing Daily...\n",
            "  ✓ Exported Daily.json (9200 rows)\n",
            "  ✓ Exported Daily_by_year.json\n",
            "Processing _InitialConditions...\n",
            "  ✓ Exported _InitialConditions.json (198 rows)\n",
            "\n",
            "Generating data dictionary...\n",
            "  ✓ Generated data_dictionary.json\n",
            "\n",
            "✓ Conversion complete! Output written to: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\n",
            "\n",
            "============================================================\n",
            "Processing: ClimAdapt_Wheat_neg31.75_117.60_585_calibrated.db\n",
            "============================================================\n",
            "Converting APSIM SQLite database: ClimAdapt_Wheat_neg31.75_117.60_585_calibrated.db\n",
            "Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\n",
            "\n",
            "Found 2 table(s): Daily, _InitialConditions\n",
            "\n",
            "Processing Daily...\n",
            "  ✓ Exported Daily.json (9200 rows)\n",
            "  ✓ Exported Daily_by_year.json\n",
            "Processing _InitialConditions...\n",
            "  ✓ Exported _InitialConditions.json (198 rows)\n",
            "\n",
            "Generating data dictionary...\n",
            "  ✓ Generated data_dictionary.json\n",
            "\n",
            "✓ Conversion complete! Output written to: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\n",
            "\n",
            "============================================================\n",
            "Processing: ClimAdapt_Wheat_neg31.75_117.60_past.db\n",
            "============================================================\n",
            "Converting APSIM SQLite database: ClimAdapt_Wheat_neg31.75_117.60_past.db\n",
            "Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\n",
            "\n",
            "Found 2 table(s): Daily, _InitialConditions\n",
            "\n",
            "Processing Daily...\n",
            "  ✓ Exported Daily.json (9100 rows)\n",
            "  ✓ Exported Daily_by_year.json\n",
            "Processing _InitialConditions...\n",
            "  ✓ Exported _InitialConditions.json (198 rows)\n",
            "\n",
            "Generating data dictionary...\n",
            "  ✓ Generated data_dictionary.json\n",
            "\n",
            "✓ Conversion complete! Output written to: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\n",
            "\n",
            "============================================================\n",
            "Batch processing complete! Processed 6 file(s).\n"
          ]
        }
      ],
      "source": [
        "# Batch process all .db files in the default directory\n",
        "input_dir = Path(DEFAULT_INPUT_DIR)\n",
        "\n",
        "if not input_dir.exists():\n",
        "    print(f\"Error: Input directory not found: {input_dir}\")\n",
        "else:\n",
        "    db_files = list(input_dir.glob(\"*.db\"))\n",
        "    \n",
        "    if not db_files:\n",
        "        print(f\"No .db files found in {input_dir}\")\n",
        "    else:\n",
        "        print(f\"Found {len(db_files)} database file(s) in {input_dir}\")\n",
        "        print()\n",
        "        \n",
        "        for db_file in db_files:\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"Processing: {db_file.name}\")\n",
        "            print(\"=\" * 60)\n",
        "            try:\n",
        "                with APSIMSQLiteConverter(str(db_file)) as converter:\n",
        "                    converter.convert()\n",
        "                print()\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {db_file.name}: {e}\")\n",
        "                print()\n",
        "                continue\n",
        "        \n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Batch processing complete! Processed {len(db_files)} file(s).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 3: Custom Input Directory\n",
        "\n",
        "Process all databases in a custom directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Input directory not found: C:\\path\\to\\your\\databases\n"
          ]
        }
      ],
      "source": [
        "# Custom input directory\n",
        "custom_input_dir = r\"C:\\path\\to\\your\\databases\"\n",
        "\n",
        "input_dir = Path(custom_input_dir)\n",
        "\n",
        "if not input_dir.exists():\n",
        "    print(f\"Error: Input directory not found: {input_dir}\")\n",
        "else:\n",
        "    db_files = list(input_dir.glob(\"*.db\"))\n",
        "    \n",
        "    if not db_files:\n",
        "        print(f\"No .db files found in {input_dir}\")\n",
        "    else:\n",
        "        print(f\"Found {len(db_files)} database file(s) in {input_dir}\")\n",
        "        print()\n",
        "        \n",
        "        for db_file in db_files:\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"Processing: {db_file.name}\")\n",
        "            print(\"=\" * 60)\n",
        "            try:\n",
        "                with APSIMSQLiteConverter(str(db_file)) as converter:\n",
        "                    converter.convert()\n",
        "                print()\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {db_file.name}: {e}\")\n",
        "                print()\n",
        "                continue\n",
        "        \n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Batch processing complete! Processed {len(db_files)} file(s).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output Structure\n",
        "\n",
        "For each database processed, JSON files are created in the same folder as the input file:\n",
        "\n",
        "- **`Daily.json`**: Daily time-step simulation outputs\n",
        "- **`Daily_by_year.json`**: Year-grouped version (if row count > 5000)\n",
        "- **`Report.json`**: Annual/checkpoint summaries\n",
        "- **`_Messages.json`**: Warnings, errors, and APSIM logs\n",
        "- **`_Simulations.json`**: Simulation metadata\n",
        "- **`_Checkpoints.json`**: Checkpoint metadata\n",
        "- **`data_dictionary.json`**: APSIM-aware column descriptions\n",
        "\n",
        "Each JSON file follows this structure:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"table\": \"Daily\",\n",
        "  \"row_count\": 3650,\n",
        "  \"columns\": {\n",
        "    \"Year\": \"integer\",\n",
        "    \"Day\": \"integer\",\n",
        "    \"Date\": \"text\",\n",
        "    \"Rain\": \"float\"\n",
        "  },\n",
        "  \"rows\": [\n",
        "    {\n",
        "      \"Year\": 2012,\n",
        "      \"Day\": 1,\n",
        "      \"Date\": \"2012-01-01\",\n",
        "      \"Rain\": 0.0\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## File Organization by Simulation\n",
        "\n",
        "This section provides functionality to automatically identify, classify, and organize database files by scenario and coordinate into simulation-specific folders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ APSIMFileOrganizer class loaded\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "class APSIMFileOrganizer:\n",
        "    \"\"\"Organizes APSIM database files by scenario and coordinate into simulation folders.\"\"\"\n",
        "    \n",
        "    # Known scenarios\n",
        "    SCENARIOS = ['245', '558', '585', 'past']\n",
        "    \n",
        "    def __init__(self, input_dir: str):\n",
        "        \"\"\"\n",
        "        Initialize organizer.\n",
        "        \n",
        "        Args:\n",
        "            input_dir: Directory containing .db files\n",
        "        \"\"\"\n",
        "        self.input_dir = Path(input_dir)\n",
        "        if not self.input_dir.exists():\n",
        "            raise FileNotFoundError(f\"Input directory not found: {input_dir}\")\n",
        "    \n",
        "    def parse_filename(self, filename: str) -> dict:\n",
        "        \"\"\"\n",
        "        Parse filename to extract scenario and coordinate information.\n",
        "        \n",
        "        Examples:\n",
        "            neg31.45_117.55_245.db -> {'coordinate': 'neg31.45_117.55', 'scenario': '245'}\n",
        "            ClimAdapt_Wheat_neg31.45_117.55_245_calibrated.db -> {'coordinate': 'neg31.45_117.55', 'scenario': '245'}\n",
        "            neg31.45_117.55_585_calibrated.db -> {'coordinate': 'neg31.45_117.55', 'scenario': '585'}\n",
        "        \n",
        "        Returns:\n",
        "            Dictionary with 'coordinate', 'scenario', and 'full_name' keys\n",
        "        \"\"\"\n",
        "        # Remove extension\n",
        "        name = Path(filename).stem\n",
        "        \n",
        "        # Strip common prefixes that might be in the filename\n",
        "        # Remove prefixes like \"ClimAdapt_Wheat_\", \"ClimAdapt_\", etc.\n",
        "        name_cleaned = name\n",
        "        prefixes_to_remove = [\n",
        "            'ClimAdapt_Wheat_',\n",
        "            'ClimAdapt_',\n",
        "            'Wheat_',\n",
        "        ]\n",
        "        for prefix in prefixes_to_remove:\n",
        "            if name_cleaned.startswith(prefix):\n",
        "                name_cleaned = name_cleaned[len(prefix):]\n",
        "        \n",
        "        # Remove common suffixes like \"_calibrated\"\n",
        "        if name_cleaned.endswith('_calibrated'):\n",
        "            name_cleaned = name_cleaned[:-len('_calibrated')]\n",
        "        \n",
        "        # Now try to find scenario and coordinate\n",
        "        scenario = None\n",
        "        coordinate = None\n",
        "        \n",
        "        # First, try to find scenario at the end (most common pattern)\n",
        "        for scen in self.SCENARIOS:\n",
        "            if name_cleaned.endswith(f'_{scen}'):\n",
        "                scenario = scen\n",
        "                coordinate = name_cleaned[:-len(f'_{scen}')].strip('_')\n",
        "                break\n",
        "            elif name_cleaned.endswith(scen) and len(name_cleaned) > len(scen):\n",
        "                # Check if it's really the scenario (not part of a number)\n",
        "                if name_cleaned[-len(scen)-1] == '_' or name_cleaned[-len(scen)-1].isalpha():\n",
        "                    scenario = scen\n",
        "                    coordinate = name_cleaned[:-len(scen)].strip('_')\n",
        "                    break\n",
        "        \n",
        "        # If scenario found, extract coordinate pattern\n",
        "        if scenario:\n",
        "            # Look for coordinate pattern: negXX.XX_XXX.XX\n",
        "            coord_match = re.search(r'(neg\\d+\\.\\d+_\\d+\\.\\d+)', coordinate)\n",
        "            if coord_match:\n",
        "                coordinate = coord_match.group(1)\n",
        "            else:\n",
        "                # If no clear coordinate pattern, use the whole thing before scenario\n",
        "                # but try to clean it up\n",
        "                coordinate = coordinate.strip('_')\n",
        "        else:\n",
        "            # No scenario found, try to extract coordinate pattern first\n",
        "            coord_match = re.search(r'(neg\\d+\\.\\d+_\\d+\\.\\d+)', name_cleaned)\n",
        "            if coord_match:\n",
        "                coordinate = coord_match.group(1)\n",
        "                # Check what comes after the coordinate\n",
        "                remaining = name_cleaned[coord_match.end():].strip('_')\n",
        "                if remaining in self.SCENARIOS:\n",
        "                    scenario = remaining\n",
        "                else:\n",
        "                    # Default to 'past' if no scenario found\n",
        "                    scenario = 'past'\n",
        "            else:\n",
        "                # Try to find coordinate pattern in parts\n",
        "                parts = name_cleaned.split('_')\n",
        "                for i in range(len(parts) - 1):\n",
        "                    potential_coord = '_'.join(parts[i:i+2])\n",
        "                    if re.match(r'neg\\d+\\.\\d+_\\d+\\.\\d+', potential_coord):\n",
        "                        coordinate = potential_coord\n",
        "                        # Check next part for scenario\n",
        "                        if i + 2 < len(parts):\n",
        "                            potential_scenario = parts[i + 2]\n",
        "                            if potential_scenario in self.SCENARIOS:\n",
        "                                scenario = potential_scenario\n",
        "                            else:\n",
        "                                scenario = 'past'\n",
        "                        else:\n",
        "                            scenario = 'past'\n",
        "                        break\n",
        "        \n",
        "        # Final fallback\n",
        "        if coordinate is None:\n",
        "            # Try one more time with the cleaned name\n",
        "            coord_match = re.search(r'(neg\\d+\\.\\d+_\\d+\\.\\d+)', name_cleaned)\n",
        "            if coord_match:\n",
        "                coordinate = coord_match.group(1)\n",
        "                scenario = 'past'  # Default scenario\n",
        "            else:\n",
        "                coordinate = name_cleaned\n",
        "                scenario = 'past'\n",
        "        \n",
        "        return {\n",
        "            'coordinate': coordinate,\n",
        "            'scenario': scenario,\n",
        "            'full_name': name,\n",
        "            'original_filename': filename\n",
        "        }\n",
        "    \n",
        "    def get_simulation_name(self, coordinate: str, scenario: str, crop: str = 'Wheat', \n",
        "                           prefix: str = 'ClimAdapt', suffix: str = 'calibrated') -> str:\n",
        "        \"\"\"\n",
        "        Generate simulation folder name.\n",
        "        \n",
        "        Format: {prefix}_{crop}_{coordinate}_{scenario}_{suffix}\n",
        "        Example: ClimAdapt_Wheat_neg31.45_117.55_245_calibrated\n",
        "        \n",
        "        Args:\n",
        "            coordinate: Coordinate string (e.g., 'neg31.45_117.55')\n",
        "            scenario: Scenario string ('245', '558', or 'past')\n",
        "            crop: Crop name (default: 'Wheat')\n",
        "            prefix: Prefix for folder name (default: 'ClimAdapt')\n",
        "            suffix: Suffix for folder name (default: 'calibrated')\n",
        "        \n",
        "        Returns:\n",
        "            Simulation folder name\n",
        "        \"\"\"\n",
        "        return f\"{prefix}_{crop}_{coordinate}_{scenario}_{suffix}\"\n",
        "    \n",
        "    def organize_files(self, move_files: bool = False, crop: str = 'Wheat', \n",
        "                      prefix: str = 'ClimAdapt', suffix: str = 'calibrated') -> dict:\n",
        "        \"\"\"\n",
        "        Organize .db files into simulation-specific folders.\n",
        "        \n",
        "        Args:\n",
        "            move_files: If True, move files; if False, copy files\n",
        "            crop: Crop name for folder naming\n",
        "            prefix: Prefix for folder name\n",
        "            suffix: Suffix for folder name\n",
        "        \n",
        "        Returns:\n",
        "            Dictionary with organization results\n",
        "        \"\"\"\n",
        "        db_files = list(self.input_dir.glob(\"*.db\"))\n",
        "        \n",
        "        if not db_files:\n",
        "            print(f\"No .db files found in {self.input_dir}\")\n",
        "            return {}\n",
        "        \n",
        "        print(f\"Found {len(db_files)} database file(s)\")\n",
        "        print()\n",
        "        \n",
        "        # Group files by simulation\n",
        "        simulation_groups = defaultdict(list)\n",
        "        file_info = {}\n",
        "        \n",
        "        for db_file in db_files:\n",
        "            parsed = self.parse_filename(db_file.name)\n",
        "            sim_name = self.get_simulation_name(\n",
        "                parsed['coordinate'], \n",
        "                parsed['scenario'],\n",
        "                crop=crop,\n",
        "                prefix=prefix,\n",
        "                suffix=suffix\n",
        "            )\n",
        "            \n",
        "            simulation_groups[sim_name].append(db_file)\n",
        "            file_info[db_file.name] = parsed\n",
        "        \n",
        "        print(f\"Identified {len(simulation_groups)} unique simulation(s):\")\n",
        "        for sim_name, files in simulation_groups.items():\n",
        "            print(f\"  - {sim_name}: {len(files)} file(s)\")\n",
        "        print()\n",
        "        \n",
        "        # Create folders and organize files\n",
        "        results = {\n",
        "            'simulations': {},\n",
        "            'total_files': len(db_files),\n",
        "            'total_simulations': len(simulation_groups)\n",
        "        }\n",
        "        \n",
        "        for sim_name, files in simulation_groups.items():\n",
        "            sim_folder = self.input_dir / sim_name\n",
        "            sim_folder.mkdir(exist_ok=True)\n",
        "            \n",
        "            print(f\"Organizing {sim_name}...\")\n",
        "            moved_files = []\n",
        "            \n",
        "            for db_file in files:\n",
        "                dest_path = sim_folder / db_file.name\n",
        "                \n",
        "                try:\n",
        "                    if move_files:\n",
        "                        if dest_path.exists():\n",
        "                            print(f\"  ⚠ Skipping {db_file.name} (already exists in destination)\")\n",
        "                        else:\n",
        "                            shutil.move(str(db_file), str(dest_path))\n",
        "                            print(f\"  ✓ Moved {db_file.name}\")\n",
        "                            moved_files.append(db_file.name)\n",
        "                    else:\n",
        "                        if dest_path.exists():\n",
        "                            print(f\"  ⚠ Skipping {db_file.name} (already exists in destination)\")\n",
        "                        else:\n",
        "                            shutil.copy2(str(db_file), str(dest_path))\n",
        "                            print(f\"  ✓ Copied {db_file.name}\")\n",
        "                            moved_files.append(db_file.name)\n",
        "                except Exception as e:\n",
        "                    print(f\"  ✗ Error processing {db_file.name}: {e}\")\n",
        "            \n",
        "            results['simulations'][sim_name] = {\n",
        "                'folder': str(sim_folder),\n",
        "                'files': moved_files,\n",
        "                'file_count': len(moved_files),\n",
        "                'coordinate': file_info[files[0].name]['coordinate'],\n",
        "                'scenario': file_info[files[0].name]['scenario']\n",
        "            }\n",
        "            print()\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def analyze_files(self) -> dict:\n",
        "        \"\"\"\n",
        "        Analyze files without organizing them.\n",
        "        \n",
        "        Returns:\n",
        "            Dictionary with analysis results\n",
        "        \"\"\"\n",
        "        db_files = list(self.input_dir.glob(\"*.db\"))\n",
        "        \n",
        "        if not db_files:\n",
        "            return {'files': [], 'simulations': {}, 'total_files': 0}\n",
        "        \n",
        "        analysis = {\n",
        "            'files': [],\n",
        "            'simulations': defaultdict(lambda: {'files': [], 'coordinates': set(), 'scenarios': set()}),\n",
        "            'total_files': len(db_files)\n",
        "        }\n",
        "        \n",
        "        for db_file in db_files:\n",
        "            parsed = self.parse_filename(db_file.name)\n",
        "            sim_name = self.get_simulation_name(\n",
        "                parsed['coordinate'],\n",
        "                parsed['scenario']\n",
        "            )\n",
        "            \n",
        "            file_data = {\n",
        "                'filename': db_file.name,\n",
        "                'coordinate': parsed['coordinate'],\n",
        "                'scenario': parsed['scenario'],\n",
        "                'simulation': sim_name\n",
        "            }\n",
        "            \n",
        "            analysis['files'].append(file_data)\n",
        "            analysis['simulations'][sim_name]['files'].append(db_file.name)\n",
        "            analysis['simulations'][sim_name]['coordinates'].add(parsed['coordinate'])\n",
        "            analysis['simulations'][sim_name]['scenarios'].add(parsed['scenario'])\n",
        "        \n",
        "        # Convert sets to lists for JSON serialization\n",
        "        for sim_name in analysis['simulations']:\n",
        "            analysis['simulations'][sim_name]['coordinates'] = list(analysis['simulations'][sim_name]['coordinates'])\n",
        "            analysis['simulations'][sim_name]['scenarios'] = list(analysis['simulations'][sim_name]['scenarios'])\n",
        "        \n",
        "        return analysis\n",
        "\n",
        "print(\"✓ APSIMFileOrganizer class loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Analyze Files (Preview Before Organizing)\n",
        "\n",
        "First, analyze the files to see how they will be classified:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total files found: 6\n",
            "Unique simulations identified: 6\n",
            "\n",
            "File Classification:\n",
            "================================================================================\n",
            "ClimAdapt_Wheat_neg31.45_117.55_245_calibrated.db  -> ClimAdapt_Wheat_neg31.45_117.55_245_calibrated\n",
            "  Coordinate: neg31.45_117.55, Scenario: 245\n",
            "ClimAdapt_Wheat_neg31.45_117.55_585_calibrated.db  -> ClimAdapt_Wheat_neg31.45_117.55_585_calibrated\n",
            "  Coordinate: neg31.45_117.55, Scenario: 585\n",
            "ClimAdapt_Wheat_neg31.45_117.55_past.db            -> ClimAdapt_Wheat_neg31.45_117.55_past_calibrated\n",
            "  Coordinate: neg31.45_117.55, Scenario: past\n",
            "ClimAdapt_Wheat_neg31.75_117.60_245_calibrated.db  -> ClimAdapt_Wheat_neg31.75_117.60_245_calibrated\n",
            "  Coordinate: neg31.75_117.60, Scenario: 245\n",
            "ClimAdapt_Wheat_neg31.75_117.60_585_calibrated.db  -> ClimAdapt_Wheat_neg31.75_117.60_585_calibrated\n",
            "  Coordinate: neg31.75_117.60, Scenario: 585\n",
            "ClimAdapt_Wheat_neg31.75_117.60_past.db            -> ClimAdapt_Wheat_neg31.75_117.60_past_calibrated\n",
            "  Coordinate: neg31.75_117.60, Scenario: past\n",
            "\n",
            "Simulation Groups:\n",
            "================================================================================\n",
            "ClimAdapt_Wheat_neg31.45_117.55_245_calibrated:\n",
            "  Files: 1\n",
            "  Coordinates: neg31.45_117.55\n",
            "  Scenarios: 245\n",
            "\n",
            "ClimAdapt_Wheat_neg31.45_117.55_585_calibrated:\n",
            "  Files: 1\n",
            "  Coordinates: neg31.45_117.55\n",
            "  Scenarios: 585\n",
            "\n",
            "ClimAdapt_Wheat_neg31.45_117.55_past_calibrated:\n",
            "  Files: 1\n",
            "  Coordinates: neg31.45_117.55\n",
            "  Scenarios: past\n",
            "\n",
            "ClimAdapt_Wheat_neg31.75_117.60_245_calibrated:\n",
            "  Files: 1\n",
            "  Coordinates: neg31.75_117.60\n",
            "  Scenarios: 245\n",
            "\n",
            "ClimAdapt_Wheat_neg31.75_117.60_585_calibrated:\n",
            "  Files: 1\n",
            "  Coordinates: neg31.75_117.60\n",
            "  Scenarios: 585\n",
            "\n",
            "ClimAdapt_Wheat_neg31.75_117.60_past_calibrated:\n",
            "  Files: 1\n",
            "  Coordinates: neg31.75_117.60\n",
            "  Scenarios: past\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Analyze files in the default directory\n",
        "organizer = APSIMFileOrganizer(DEFAULT_INPUT_DIR)\n",
        "analysis = organizer.analyze_files()\n",
        "\n",
        "print(f\"Total files found: {analysis['total_files']}\")\n",
        "print(f\"Unique simulations identified: {len(analysis['simulations'])}\")\n",
        "print()\n",
        "print(\"File Classification:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for file_data in analysis['files'][:20]:  # Show first 20 files\n",
        "    print(f\"{file_data['filename']:50s} -> {file_data['simulation']}\")\n",
        "    print(f\"  Coordinate: {file_data['coordinate']}, Scenario: {file_data['scenario']}\")\n",
        "\n",
        "if len(analysis['files']) > 20:\n",
        "    print(f\"\\n... and {len(analysis['files']) - 20} more file(s)\")\n",
        "\n",
        "print()\n",
        "print(\"Simulation Groups:\")\n",
        "print(\"=\" * 80)\n",
        "for sim_name, sim_data in analysis['simulations'].items():\n",
        "    print(f\"{sim_name}:\")\n",
        "    print(f\"  Files: {len(sim_data['files'])}\")\n",
        "    print(f\"  Coordinates: {', '.join(sim_data['coordinates'])}\")\n",
        "    print(f\"  Scenarios: {', '.join(sim_data['scenarios'])}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Organize Files into Simulation Folders\n",
        "\n",
        "Choose one of the options below:\n",
        "\n",
        "- **Copy files** (recommended): Creates copies in organized folders, keeps originals\n",
        "- **Move files**: Moves files to organized folders (removes from original location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6 database file(s)\n",
            "\n",
            "Identified 6 unique simulation(s):\n",
            "  - ClimAdapt_Wheat_neg31.45_117.55_245_calibrated: 1 file(s)\n",
            "  - ClimAdapt_Wheat_neg31.45_117.55_585_calibrated: 1 file(s)\n",
            "  - ClimAdapt_Wheat_neg31.45_117.55_past_calibrated: 1 file(s)\n",
            "  - ClimAdapt_Wheat_neg31.75_117.60_245_calibrated: 1 file(s)\n",
            "  - ClimAdapt_Wheat_neg31.75_117.60_585_calibrated: 1 file(s)\n",
            "  - ClimAdapt_Wheat_neg31.75_117.60_past_calibrated: 1 file(s)\n",
            "\n",
            "Organizing ClimAdapt_Wheat_neg31.45_117.55_245_calibrated...\n",
            "  ✓ Copied ClimAdapt_Wheat_neg31.45_117.55_245_calibrated.db\n",
            "\n",
            "Organizing ClimAdapt_Wheat_neg31.45_117.55_585_calibrated...\n",
            "  ✓ Copied ClimAdapt_Wheat_neg31.45_117.55_585_calibrated.db\n",
            "\n",
            "Organizing ClimAdapt_Wheat_neg31.45_117.55_past_calibrated...\n",
            "  ✓ Copied ClimAdapt_Wheat_neg31.45_117.55_past.db\n",
            "\n",
            "Organizing ClimAdapt_Wheat_neg31.75_117.60_245_calibrated...\n",
            "  ✓ Copied ClimAdapt_Wheat_neg31.75_117.60_245_calibrated.db\n",
            "\n",
            "Organizing ClimAdapt_Wheat_neg31.75_117.60_585_calibrated...\n",
            "  ✓ Copied ClimAdapt_Wheat_neg31.75_117.60_585_calibrated.db\n",
            "\n",
            "Organizing ClimAdapt_Wheat_neg31.75_117.60_past_calibrated...\n",
            "  ✓ Copied ClimAdapt_Wheat_neg31.75_117.60_past.db\n",
            "\n",
            "================================================================================\n",
            "Organization Summary:\n",
            "Total files processed: 6\n",
            "Simulations created: 6\n",
            "\n",
            "ClimAdapt_Wheat_neg31.45_117.55_245_calibrated:\n",
            "  Folder: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.45_117.55_245_calibrated\n",
            "  Files: 1\n",
            "  Coordinate: neg31.45_117.55\n",
            "  Scenario: 245\n",
            "\n",
            "ClimAdapt_Wheat_neg31.45_117.55_585_calibrated:\n",
            "  Folder: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.45_117.55_585_calibrated\n",
            "  Files: 1\n",
            "  Coordinate: neg31.45_117.55\n",
            "  Scenario: 585\n",
            "\n",
            "ClimAdapt_Wheat_neg31.45_117.55_past_calibrated:\n",
            "  Folder: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.45_117.55_past_calibrated\n",
            "  Files: 1\n",
            "  Coordinate: neg31.45_117.55\n",
            "  Scenario: past\n",
            "\n",
            "ClimAdapt_Wheat_neg31.75_117.60_245_calibrated:\n",
            "  Folder: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.75_117.60_245_calibrated\n",
            "  Files: 1\n",
            "  Coordinate: neg31.75_117.60\n",
            "  Scenario: 245\n",
            "\n",
            "ClimAdapt_Wheat_neg31.75_117.60_585_calibrated:\n",
            "  Folder: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.75_117.60_585_calibrated\n",
            "  Files: 1\n",
            "  Coordinate: neg31.75_117.60\n",
            "  Scenario: 585\n",
            "\n",
            "ClimAdapt_Wheat_neg31.75_117.60_past_calibrated:\n",
            "  Folder: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.75_117.60_past_calibrated\n",
            "  Files: 1\n",
            "  Coordinate: neg31.75_117.60\n",
            "  Scenario: past\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# OPTION A: Copy files (keeps originals in place)\n",
        "# This is the recommended option for safety\n",
        "\n",
        "organizer = APSIMFileOrganizer(DEFAULT_INPUT_DIR)\n",
        "results = organizer.organize_files(\n",
        "    move_files=False,  # Copy instead of move\n",
        "    crop='Wheat',      # Change if needed\n",
        "    prefix='ClimAdapt', # Change if needed\n",
        "    suffix='calibrated' # Change if needed\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Organization Summary:\")\n",
        "print(f\"Total files processed: {results['total_files']}\")\n",
        "print(f\"Simulations created: {results['total_simulations']}\")\n",
        "print()\n",
        "for sim_name, sim_data in results['simulations'].items():\n",
        "    print(f\"{sim_name}:\")\n",
        "    print(f\"  Folder: {sim_data['folder']}\")\n",
        "    print(f\"  Files: {sim_data['file_count']}\")\n",
        "    print(f\"  Coordinate: {sim_data['coordinate']}\")\n",
        "    print(f\"  Scenario: {sim_data['scenario']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTION B: Move files (removes from original location)\n",
        "# Use with caution - this will move files from the original location\n",
        "\n",
        "# Uncomment the lines below to use this option:\n",
        "# organizer = APSIMFileOrganizer(DEFAULT_INPUT_DIR)\n",
        "# results = organizer.organize_files(\n",
        "#     move_files=True,  # Move instead of copy\n",
        "#     crop='Wheat',\n",
        "#     prefix='ClimAdapt',\n",
        "#     suffix='calibrated'\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Convert Organized Files to JSON\n",
        "\n",
        "After organizing files, you can convert each simulation's databases to JSON. The JSON files will be created in each simulation's folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 6 simulation folder(s)\n",
            "\n",
            "================================================================================\n",
            "Processing simulation: ClimAdapt_Wheat_neg31.45_117.55_245_calibrated\n",
            "================================================================================\n",
            "Found 1 database file(s) in this folder\n",
            "\n",
            "Converting: ClimAdapt_Wheat_neg31.45_117.55_245_calibrated.db\n",
            "Converting APSIM SQLite database: ClimAdapt_Wheat_neg31.45_117.55_245_calibrated.db\n",
            "Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.45_117.55_245_calibrated\n",
            "\n",
            "Found 7 table(s): Daily, Report, _Checkpoints, _InitialConditions, _Messages, _Simulations, _Units\n",
            "\n",
            "Processing Daily...\n",
            "  ✓ Exported Daily.json (10574 rows)\n",
            "  ✓ Exported Daily_by_year.json\n",
            "Processing Report...\n",
            "  ✓ Exported Report.json (27 rows)\n",
            "Processing _Checkpoints...\n",
            "  ✓ Exported _Checkpoints.json (1 rows)\n",
            "Processing _InitialConditions...\n",
            "  ✓ Exported _InitialConditions.json (198 rows)\n",
            "Processing _Messages...\n",
            "  ✓ Exported _Messages.json (1936 rows)\n",
            "Processing _Simulations...\n",
            "  ✓ Exported _Simulations.json (1 rows)\n",
            "Processing _Units...\n",
            "  ✓ Exported _Units.json (2 rows)\n",
            "\n",
            "Generating data dictionary...\n",
            "  ✓ Generated data_dictionary.json\n",
            "\n",
            "✓ Conversion complete! Output written to: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.45_117.55_245_calibrated\n",
            "\n",
            "✓ Completed processing ClimAdapt_Wheat_neg31.45_117.55_245_calibrated\n",
            "\n",
            "================================================================================\n",
            "Processing simulation: ClimAdapt_Wheat_neg31.45_117.55_585_calibrated\n",
            "================================================================================\n",
            "Found 1 database file(s) in this folder\n",
            "\n",
            "Converting: ClimAdapt_Wheat_neg31.45_117.55_585_calibrated.db\n",
            "Converting APSIM SQLite database: ClimAdapt_Wheat_neg31.45_117.55_585_calibrated.db\n",
            "Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.45_117.55_585_calibrated\n",
            "\n",
            "Found 7 table(s): Daily, Report, _Checkpoints, _InitialConditions, _Messages, _Simulations, _Units\n",
            "\n",
            "Processing Daily...\n",
            "  ✓ Exported Daily.json (10574 rows)\n",
            "  ✓ Exported Daily_by_year.json\n",
            "Processing Report...\n",
            "  ✓ Exported Report.json (26 rows)\n",
            "Processing _Checkpoints...\n",
            "  ✓ Exported _Checkpoints.json (1 rows)\n",
            "Processing _InitialConditions...\n",
            "  ✓ Exported _InitialConditions.json (198 rows)\n",
            "Processing _Messages...\n",
            "  ✓ Exported _Messages.json (2175 rows)\n",
            "Processing _Simulations...\n",
            "  ✓ Exported _Simulations.json (1 rows)\n",
            "Processing _Units...\n",
            "  ✓ Exported _Units.json (2 rows)\n",
            "\n",
            "Generating data dictionary...\n",
            "  ✓ Generated data_dictionary.json\n",
            "\n",
            "✓ Conversion complete! Output written to: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.45_117.55_585_calibrated\n",
            "\n",
            "✓ Completed processing ClimAdapt_Wheat_neg31.45_117.55_585_calibrated\n",
            "\n",
            "================================================================================\n",
            "Processing simulation: ClimAdapt_Wheat_neg31.45_117.55_past_calibrated\n",
            "================================================================================\n",
            "Found 1 database file(s) in this folder\n",
            "\n",
            "Converting: ClimAdapt_Wheat_neg31.45_117.55_past.db\n",
            "Converting APSIM SQLite database: ClimAdapt_Wheat_neg31.45_117.55_past.db\n",
            "Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.45_117.55_past_calibrated\n",
            "\n",
            "Found 7 table(s): Daily, Report, _Checkpoints, _InitialConditions, _Messages, _Simulations, _Units\n",
            "\n",
            "Processing Daily...\n",
            "  ✓ Exported Daily.json (9114 rows)\n",
            "  ✓ Exported Daily_by_year.json\n",
            "Processing Report...\n",
            "  ✓ Exported Report.json (24 rows)\n",
            "Processing _Checkpoints...\n",
            "  ✓ Exported _Checkpoints.json (1 rows)\n",
            "Processing _InitialConditions...\n",
            "  ✓ Exported _InitialConditions.json (198 rows)\n",
            "Processing _Messages...\n",
            "  ✓ Exported _Messages.json (2039 rows)\n",
            "Processing _Simulations...\n",
            "  ✓ Exported _Simulations.json (1 rows)\n",
            "Processing _Units...\n",
            "  ✓ Exported _Units.json (2 rows)\n",
            "\n",
            "Generating data dictionary...\n",
            "  ✓ Generated data_dictionary.json\n",
            "\n",
            "✓ Conversion complete! Output written to: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.45_117.55_past_calibrated\n",
            "\n",
            "✓ Completed processing ClimAdapt_Wheat_neg31.45_117.55_past_calibrated\n",
            "\n",
            "================================================================================\n",
            "Processing simulation: ClimAdapt_Wheat_neg31.75_117.60_245_calibrated\n",
            "================================================================================\n",
            "Found 1 database file(s) in this folder\n",
            "\n",
            "Converting: ClimAdapt_Wheat_neg31.75_117.60_245_calibrated.db\n",
            "Converting APSIM SQLite database: ClimAdapt_Wheat_neg31.75_117.60_245_calibrated.db\n",
            "Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.75_117.60_245_calibrated\n",
            "\n",
            "Found 2 table(s): Daily, _InitialConditions\n",
            "\n",
            "Processing Daily...\n",
            "  ✓ Exported Daily.json (9200 rows)\n",
            "  ✓ Exported Daily_by_year.json\n",
            "Processing _InitialConditions...\n",
            "  ✓ Exported _InitialConditions.json (198 rows)\n",
            "\n",
            "Generating data dictionary...\n",
            "  ✓ Generated data_dictionary.json\n",
            "\n",
            "✓ Conversion complete! Output written to: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.75_117.60_245_calibrated\n",
            "\n",
            "✓ Completed processing ClimAdapt_Wheat_neg31.75_117.60_245_calibrated\n",
            "\n",
            "================================================================================\n",
            "Processing simulation: ClimAdapt_Wheat_neg31.75_117.60_585_calibrated\n",
            "================================================================================\n",
            "Found 1 database file(s) in this folder\n",
            "\n",
            "Converting: ClimAdapt_Wheat_neg31.75_117.60_585_calibrated.db\n",
            "Converting APSIM SQLite database: ClimAdapt_Wheat_neg31.75_117.60_585_calibrated.db\n",
            "Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.75_117.60_585_calibrated\n",
            "\n",
            "Found 2 table(s): Daily, _InitialConditions\n",
            "\n",
            "Processing Daily...\n",
            "  ✓ Exported Daily.json (9200 rows)\n",
            "  ✓ Exported Daily_by_year.json\n",
            "Processing _InitialConditions...\n",
            "  ✓ Exported _InitialConditions.json (198 rows)\n",
            "\n",
            "Generating data dictionary...\n",
            "  ✓ Generated data_dictionary.json\n",
            "\n",
            "✓ Conversion complete! Output written to: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.75_117.60_585_calibrated\n",
            "\n",
            "✓ Completed processing ClimAdapt_Wheat_neg31.75_117.60_585_calibrated\n",
            "\n",
            "================================================================================\n",
            "Processing simulation: ClimAdapt_Wheat_neg31.75_117.60_past_calibrated\n",
            "================================================================================\n",
            "Found 1 database file(s) in this folder\n",
            "\n",
            "Converting: ClimAdapt_Wheat_neg31.75_117.60_past.db\n",
            "Converting APSIM SQLite database: ClimAdapt_Wheat_neg31.75_117.60_past.db\n",
            "Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.75_117.60_past_calibrated\n",
            "\n",
            "Found 2 table(s): Daily, _InitialConditions\n",
            "\n",
            "Processing Daily...\n",
            "  ✓ Exported Daily.json (9100 rows)\n",
            "  ✓ Exported Daily_by_year.json\n",
            "Processing _InitialConditions...\n",
            "  ✓ Exported _InitialConditions.json (198 rows)\n",
            "\n",
            "Generating data dictionary...\n",
            "  ✓ Generated data_dictionary.json\n",
            "\n",
            "✓ Conversion complete! Output written to: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\APSIM files\\ClimAdapt_Wheat_neg31.75_117.60_past_calibrated\n",
            "\n",
            "✓ Completed processing ClimAdapt_Wheat_neg31.75_117.60_past_calibrated\n",
            "\n",
            "================================================================================\n",
            "All simulations processed!\n"
          ]
        }
      ],
      "source": [
        "# Convert all databases in organized simulation folders to JSON\n",
        "input_dir = Path(DEFAULT_INPUT_DIR)\n",
        "\n",
        "# Find all simulation folders (folders containing .db files)\n",
        "simulation_folders = [d for d in input_dir.iterdir() \n",
        "                     if d.is_dir() and list(d.glob(\"*.db\"))]\n",
        "\n",
        "print(f\"Found {len(simulation_folders)} simulation folder(s)\")\n",
        "print()\n",
        "\n",
        "for sim_folder in simulation_folders:\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Processing simulation: {sim_folder.name}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    db_files = list(sim_folder.glob(\"*.db\"))\n",
        "    print(f\"Found {len(db_files)} database file(s) in this folder\")\n",
        "    print()\n",
        "    \n",
        "    for db_file in db_files:\n",
        "        print(f\"Converting: {db_file.name}\")\n",
        "        try:\n",
        "            # Output will be in the same folder as the .db file\n",
        "            with APSIMSQLiteConverter(str(db_file)) as converter:\n",
        "                converter.convert()\n",
        "            print()\n",
        "        except Exception as e:\n",
        "            print(f\"Error converting {db_file.name}: {e}\")\n",
        "            print()\n",
        "            continue\n",
        "    \n",
        "    print(f\"✓ Completed processing {sim_folder.name}\")\n",
        "    print()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"All simulations processed!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
